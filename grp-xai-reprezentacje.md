# Grupa: Analiza i Interpretowalność Reprezentacji w Nowoczesnych Modelach Głębokiego Uczenia

## Prowadzący
Vladimir Zaigrajew

## Opis
W ramach warsztatów badawczych studenci będą uczestniczyć w projekcie dotyczącym nowoczesnych metod uczenia reprezentacji w głębokich sieciach neuronowych. Podczas zajęć zapoznają się z technikami tworzenia reprezentacji tekstowych i obrazowych, zarówno w podejściu uczenia nadzorowanego, jak i nienadzorowanego. Studenci poznają praktyczne zastosowania i metody oceny tych technik w zadaniach takich jak wyszukiwanie podobieństw (similarity search), ekstrakcja cech na potrzeby klasyfikacji, wykorzystanie w modelach typu vLLM czy zastosowania do dziedziny interpretowalności modeli.

## Projekty
Projekty będą realizowane w grupach 3-5 osobowych. Tematyka projektów obejmuje:
* Ulepszanie technik uczenia reprezentacji
* Analiza lub interpretacja reprezentacji w aktualnych modelach deep learningowych
* Interpretowalność w uczeniu reprezentacji
* Nowe zastosowania modeli z dziedziny uczenia reprezentacji
Szczegółowy zakres projektów oraz przydział studentów do grup zostanie ustalony podczas zajęć.

## Zasady prezentacji
* Czas trwania: 20-30 minut + 5 minut na pytania
* Prezentacje powinny być zrozumiałe dla wszystkich studentów
* Wymagane jest cytowanie źródeł dla wszystkich wykorzystanych rysunków i cytatów (z wyjątkiem materiałów własnych)
* Zachęcamy do wykorzystywania materiałów graficznych
* 
## Kontakt
W przypadku:
* chęci zaprezentowania innego artykułu związanego z tematyką kursu
* problemów z dostępem do materiałów
prosimy o kontakt

## Proponowane artykuły
Podkreślam że podane artykuły mogą być brane pod uwagę ale również wzorując się na nich zachęcam studentów do samego znalezienia i wyboru artykułu po uzgodnieniu ze mną

- Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, Saining Xie; “Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think”; https://openreview.net/forum?id=DJSZGGZYVi
- Mu Cai, Jianwei Yang, Jianfeng Gao, Yong Jae Lee; “Matryoshka Multimodal Models”; 2024 ; https://matryoshka-mm.github.io
- Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, Wei Ping; “MM-EMBED: UNIVERSAL MULTIMODAL RETRIEVAL WITH MULTIMODAL LLMS”; 2024; https://openreview.net/forum?id=i45NQb2iKO
- Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman; “With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations”; 2021; https://openaccess.thecvf.com/content/ICCV2021/html/Dwibedi_With_a_Little_Help_From_My_Friends_Nearest-Neighbor_Contrastive_Learning_ICCV_2021_paper.html
- Jonathan Crabbé, Pau Rodríguez, Vaishaal Shankar, Luca Zappella, Arno Blaas; “Interpreting CLIP: Insights on the Robustness to ImageNet Distribution Shifts”; 2024; https://openreview.net/forum?id=1SCptTFtmV
- Steven Bills∗, Nick Cammarata∗, Dan Mossing∗, Henk Tillman∗, Leo Gao∗, Gabriel Goh, Ilya Sutskever,Jan Leike, Jeff Wu∗, William Saunders∗; Language models can explain neurons in language models; 2023; https://openai.com/index/language-models-can-explain-neurons-in-language-models/
- Tuomas Oikarinen, Tsui-Wei Weng; “CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks”; 2023; https://openreview.net/forum?id=iPWiwWHc1V
- Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, Rory Sayres; “Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)“; 2018; https://proceedings.mlr.press/v80/kim18d.html
- Adly Templeton, Tom Conerly*, Jonathan Marcus, Jack Lindsey, et al.; “Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet”; 2024; https://transformer-circuits.pub/2024/scaling-monosemanticity/
- Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio P. Calmon, Himabindu Lakkaraju; “Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)“; 2024; https://openreview.net/forum?id=7UyBKTFrtd¬eId=LGFAXWnNwd
- Pedro Madeira; André Carreiro; Alex Gaudio; Luís Rosado; Filipe Soares; Asim Smailagic; “ZEBRA: Explaining Rare Cases Through Outlying Interpretable Concepts”; 2023; https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Madeira_ZEBRA_Explaining_Rare_Cases_Through_Outlying_Interpretable_Concepts_CVPRW_2023_paper.html
- Ahmed Abdulaal, Hugo Fry, Nina Montaña-Brown, Ayodeji Ijishakin, Jack Gao, Stephanie Hyland, Daniel C. Alexander, Daniel C. Castro; An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation; 2024; https://openreview.net/forum?id=ZLAQ6Pjf9y
- INTERPRETING AND STEERING LLM REPRESENTATIONS WITH MUTUAL INFORMATION-BASED EXPLANATIONS ON SPARSE AUTOENCODERS; (2025)?; https://openreview.net/forum?id=vc1i3a4O99 (Ten artykuł mimo braku jeszcze publikacji został tutaj dodany jako inspiracja tematyczności artykułów)
